n1<-length(x1)
n2<-length(x2)
xd+c(-1,1)*qt(.975,n1-1)*sd/sqrt(n1) #n1=n2 for paired!
mean(xd)+c(-1,1)*qt(.975,n1-1)*sd/sqrt(n1) #n1=n2 for paired!
t.test(x1,x2,paired=T,var.equal=TRUE)$conf
t.test(x1,x2,paired=TRUE)$conf #meme resultat
mean(xd)+c(-1,1)*qt(.975,n1-1)*sd/sqrt(n1) #n1=n2 for paired!
t.test(x1,x2,paired=T,var.equal=TRUE)$conf
t.test(x1,x2,paired=TRUE)$conf #meme resultat
n1<-length(x1)
n2<-length(x2)
sp<-sqrt(((n1-1)*sd(x1)^2+(n2-1)*sd(x2)^2)/(n1+n2-2)) #equal variance: pooled variance estimator
se<-sqrt(sd(x1)^2/n1 + sd(x2)^2/n2) #Unequal variances
md<-mean(x1)-mean(x2)
semd<-sp*sqrt(1/n1+1/n2) # = se (ici?)
se<-sqrt(2^2/100 + 0.5^2/100) #Unequal variances
md<-6-4
md+c(-1,1)*qt(.975,n1+n2-2)*se
md+c(-1,1)*qt(.975,200-2)*se
qnorm(0.975)
md+c(-1,1)*qnorm(0.975)*se
n1<-9
n2<-9
sp<-sqrt(((n1-1)*1.5^2+(n2-1)*1.8^2)/(n1+n2-2)) #equal variance: pooled variance estimator
sp<-sqrt(((n1-1)*1.5^2+(n2-1)*1.8^2)/(n1+n2-2)) #equal variance: pooled variance estimator
md<- -4
semd<-sp*sqrt(1/n1+1/n2) # = se (ici?)
md+c(-1,1)*qt(.975,n1+n2-2)*semd
md+c(-1,1)*qt(.95,n1+n2-2)*semd
se<-sqrt(1.5^2/n1 + 1.8^2/n2) #Unequal variances
1100+qt(0.975,9)*30/3
1100+c(-1,1)*qt(0.975,9)*30/3
1100+c(-1,1)*qt(0.975,9-1)*30/3
qt(0.975,9-1)
qt(0.975,98)
1100+c(-1,1)*qt(0.975,9-1)*30/3
install.packages("caret")
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
View(predictors)
View(allx)
data(AlzheimerDisease)
adData = data.frame(predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(Hmisc)
View(concrete)
View(mixtures)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
View(mixtures)
summary(training)
str(training)
plot(training$CompressiveStrength)
library(ggplot2)
qplot(names(training)[1],data=training)
names(training)[1]
qplot(cement,data=training)
qplot(cement,data=training,geom="density")
names(training)
qplot(Cement,data=training,geom="density")
qplot(Cement,data=training)
qplot(Cement,data=training,geom="density")
qplot(Cement,data=training)
qplot(CompressiveStrength,data=training)
plot(training$CompressiveStrength)
plot(training$CompressiveStrength, col=Cement)
plot(training$CompressiveStrength)
plot(training$CompressiveStrength, col=Age)
library(Hmisc)
names(training)
cutAge <- cut2(training$Age,g=5)
table(cutAage)
table(cutAge)
qplot(CompressiveStrength,data=training, col=cutAge)
h
plot(training$CompressiveStrength)
plot(training$CompressiveStrength, col=cutAge)
cutAge <- cut2(training$Age,g=3)
table(cutAge)
plot(training$CompressiveStrength, col=cutAge)
cutAge <- cut2(training$Age,g=5)
table(cutAge)
plot(training$CompressiveStrength, col=cutAge)
cutFlyAsh <- cut2(training$FlyAsh,g=5)
table(cutFlyAsh)
plot(training$CompressiveStrength, col=cutFlyAsh)
plot(training$CompressiveStrength, col=cutAge)
plot(concrete$CompressiveStrength, col=cutAge)
plot(training$CompressiveStrength, col=cutAge)
plot(training$CompressiveStrength, col=cutFlyAsh)
plot(training$CompressiveStrength)
plot(training$Superplasticizer)
qplot(training$Superplasticizer)
qplot(log(1+training$Superplasticizer)
)
qplot(log(1+training$Superplasticizer))
qplot(training$Superplasticizer)
qplot(log(1+training$Superplasticizer))
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
names(training)
grep("IL*",names(training),value=T)
grep("IL",names(training),value=T)
grep("^IL",names(training),value=T)
grep("^IL",names(training),value=T,fixed=T)
grep("IL",names(training),value=T,fixed=T)
grep("^IL",names(training),value=T)
grep("^IL",names(training))
ILindexes <- grep("^IL",names(training))
preProc <- preProcess(training[,ILindexes],method="pca",thresh=0.9)
trainPC <- predict(preProc,training[,ILindexes])
preProc <- preProcess(training[,ILindexes],method="pca",thresh=0.5)
preProc <- preProcess(training[,ILindexes],method="pca",thresh=0.99)
trainPC <- predict(preProc,training[,ILindexes])
preProc <- preProcess(training[,ILindexes],method="pca",thresh=0.9)
trainPC <- predict(preProc,training[,ILindexes])
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ILindexes <- grep("^IL",names(training))
ILtraining = training[,ILindexes]
preProc <- preProcess(ILtraining,method="pca",thresh=0.8)
trainPC <- predict(preProc,ILtraining)
modelPCFit <- train(training$diagnosis ~ .,method="glm",data=trainPC)
library(caret)
modelPCFit <- train(training$diagnosis ~ .,method="glm",data=trainPC)
library(e1071)
trainPC <- predict(preProc,ILtraining)
install.packages("e1071")
library(e1071)
modelPCFit <- train(training$diagnosis ~ .,method="glm",data=trainPC)
modelFit <- train(training$diagnosis ~ .,method="glm",data=ILtraining)
testPC <- predict(preProc,testing[,ILindexes])
testPC <- predict(preProc,testing[,ILindexes])
confusionMatrix(testing$diagnosis,predict(modelPCFit,testPC))
modelFit <- train(training$diagnosis ~ .,method="glm",data=ILtraining)
modelFit <- train(training$diagnosis ~ .,method="glm",data=ILtraining)
confusionMatrix(testing$diagnosis,predict(modelFit,testing[,ILindexes]))
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
names(training)
ILindexes <- grep("^IL",names(training))
preProc <- preProcess(training[,ILindexes],method="pca",thresh=0.8)
trainPC <- predict(preProc,training[,ILindexes])
# We start by defining the quantities whose value is known.
S0=8; # Equity at time 0, i.e. today.
sigmaS=0.75; # Instantaneous volatility of equity
r=0.06; # Risk-free rate on the market
T=1; # Maturity
B=7.5; # Face value of debt obligation, i.e. liabilities.
# We then need to write down the function we will minimize in order to obtain
# V0, the value of companyâs assets today, and sigmaV, assetsâ volatility
# To write a function in R, we use the command âfunctionâ. Refer to the R intro
# for more details.
Merton_solve=function(parm){
V0=parm[1] #initial value for V0
sigmaV=parm[2] #initial value for sigmaV
# And now, all the quantities we have seen in the slides.
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
F=V0*pnorm(d1)-B*exp(-r*T)*pnorm(d2)-S0
G=pnorm(d1)*sigmaV*V0-sigmaS*S0
#  Finally the result of our function:
return(F^2+G^2)
}
# For the minimization step, in order to find V_0 and sigma_V,
# we need to specify two initial values.
# Let us choose V_0=13, and sigmaV=0.5.
# Other plausible values can obviously be chosen.
solutions=optim(c(V0=13,sigmaV=0.5),Merton_solve)
# What are the estimated values?
V0=solutions$par[1]
sigmaV=solutions$par[2]
# Let us compute d1 and d2 explicitly:
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
# And finally the probability of default in one year for our company
pnorm(-d2)
# We start by defining the quantities whose value is known.
S0=8; # Equity at time 0, i.e. today.
sigmaS=0.75; # Instantaneous volatility of equity
r=0.06; # Risk-free rate on the market
T=1; # Maturity
B=14; # Face value of debt obligation, i.e. liabilities.
# We then need to write down the function we will minimize in order to obtain
# V0, the value of companyâs assets today, and sigmaV, assetsâ volatility
# To write a function in R, we use the command âfunctionâ. Refer to the R intro
# for more details.
Merton_solve=function(parm){
V0=parm[1] #initial value for V0
sigmaV=parm[2] #initial value for sigmaV
# And now, all the quantities we have seen in the slides.
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
F=V0*pnorm(d1)-B*exp(-r*T)*pnorm(d2)-S0
G=pnorm(d1)*sigmaV*V0-sigmaS*S0
#  Finally the result of our function:
return(F^2+G^2)
}
# For the minimization step, in order to find V_0 and sigma_V,
# we need to specify two initial values.
# Let us choose V_0=13, and sigmaV=0.5.
# Other plausible values can obviously be chosen.
solutions=optim(c(V0=13,sigmaV=0.5),Merton_solve)
# What are the estimated values?
V0=solutions$par[1]
sigmaV=solutions$par[2]
# Let us compute d1 and d2 explicitly:
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
# And finally the probability of default in one year for our company
pnorm(-d2)
# We start by defining the quantities whose value is known.
S0=11; # Equity at time 0, i.e. today.
sigmaS=0.7; # Instantaneous volatility of equity
r=0.06; # Risk-free rate on the market
T=1; # Maturity
B=18; # Face value of debt obligation, i.e. liabilities.
# We then need to write down the function we will minimize in order to obtain
# V0, the value of companyâs assets today, and sigmaV, assetsâ volatility
# To write a function in R, we use the command âfunctionâ. Refer to the R intro
# for more details.
Merton_solve=function(parm){
V0=parm[1] #initial value for V0
sigmaV=parm[2] #initial value for sigmaV
# And now, all the quantities we have seen in the slides.
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
F=V0*pnorm(d1)-B*exp(-r*T)*pnorm(d2)-S0
G=pnorm(d1)*sigmaV*V0-sigmaS*S0
#  Finally the result of our function:
return(F^2+G^2)
}
# For the minimization step, in order to find V_0 and sigma_V,
# we need to specify two initial values.
# Let us choose V_0=13, and sigmaV=0.5.
# Other plausible values can obviously be chosen.
solutions=optim(c(V0=13,sigmaV=0.5),Merton_solve)
# What are the estimated values?
V0=solutions$par[1]
sigmaV=solutions$par[2]
# Let us compute d1 and d2 explicitly:
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
# And finally the probability of default in one year for our company
pnorm(-d2)
# We start by defining the quantities whose value is known.
S0=11; # Equity at time 0, i.e. today.
sigmaS=0.7; # Instantaneous volatility of equity
r=0.06; # Risk-free rate on the market
T=1; # Maturity
B=15; # Face value of debt obligation, i.e. liabilities.
# We then need to write down the function we will minimize in order to obtain
# V0, the value of companyâs assets today, and sigmaV, assetsâ volatility
# To write a function in R, we use the command âfunctionâ. Refer to the R intro
# for more details.
Merton_solve=function(parm){
V0=parm[1] #initial value for V0
sigmaV=parm[2] #initial value for sigmaV
# And now, all the quantities we have seen in the slides.
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
F=V0*pnorm(d1)-B*exp(-r*T)*pnorm(d2)-S0
G=pnorm(d1)*sigmaV*V0-sigmaS*S0
#  Finally the result of our function:
return(F^2+G^2)
}
# For the minimization step, in order to find V_0 and sigma_V,
# we need to specify two initial values.
# Let us choose V_0=13, and sigmaV=0.5.
# Other plausible values can obviously be chosen.
solutions=optim(c(V0=13,sigmaV=0.5),Merton_solve)
# What are the estimated values?
V0=solutions$par[1]
sigmaV=solutions$par[2]
# Let us compute d1 and d2 explicitly:
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
# And finally the probability of default in one year for our company
pnorm(-d2)
qnorm(2/100)
pnorm(2.05)
pnorm(-2.05)
d1=2.053749+0.3
d1
sigmaV=0.3
T=1
B=15
r=0.06
B*exp((d1*sigmaV*sqrt(T))-(r+sigmaV^2/2)*T))
B*exp((d1*sigmaV*sqrt(T))-(r+sigmaV^2/2)*T)
install.packages("CreditMetrics")
qnorm(0.0019)
qnorm(0.9995)
50*0.2*((pnorm((qnorm(0.5)))-0.005)
)
50*0.2*(pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.9999)/sqrt(1-0.15)))-0.005)
50*0.2*(pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.9999)/sqrt(1-0.15)))-0.005)*0.08
50*0.2*(pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.9999))/sqrt(1-0.15))-0.005)
50*0.2*(pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.9999))/sqrt(1-0.15))-0.005)*0.08
12.5*50*0.2*(pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.9999))/sqrt(1-0.15))-0.005)
pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.9999))/sqrt(1-0.15))
pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.999))/sqrt(1-0.15))
1/0.08
50*0.2*(pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.999))/sqrt(1-0.15))-0.005)
50*0.2*(pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.999))/sqrt(1-0.15))-0.005)*12.5
qnorm(0.06)
qnorm(1-0.06-2.82)
qnorm(0.0006)
qnorm(1-0.0006-0.00282)
qnorm(1-0.0006)
qnorm(1-0.0006-0.0282)
rho_retail = 0.03+0.13*exp(-35*0.01)
wcdr=pnorm((qnorm(0.01)+sqrt(rho_retail)*qnorm(0.999))/sqrt(1-rho_retail))
wcdr
200*0.7*(wcdr-0.01)
200*0.7*(wcdr-0.01)*12.5
rho_corp = 0.12*(1+exp(-50*0.003))
wcdr=pnorm((qnorm(0.003)+sqrt(rho_corp)*qnorm(0.999))/sqrt(1-rho_corp))
rho_corp
wcdr
b=(0.11852 - 0.05478*log(0.003))^2
ma=(1+(3-2.5)*b)/(1-1.5*b)
500*0.6*(wcdr-0.003)*ma
500*0.6*(wcdr-0.003)*ma*12.5
(1/100)/0.4
(7/100)/0.4
(0.6/100)/0.4
(0.025*10-5*0.015)/5
# Let's generate 100 losses
losses=rlnorm(100,1,2)
# or
# losses=read.table("losses_svar.txt",head=T),
# and then
# losses=losses$x
# if you use my data.
# We can plot the losses
hist(losses, col=3)
quantile(losses,0.95,type=3)
losses=sort(losses)
# Now we select the worst 50%
worst_losses=losses[51:100]
# The 95% S-VaR is therefore
quantile(worst_losses,0.95,type=3)
quantile(losses,0.975,type=3)
time()
date()
setwd("C:/Users/HomeUser/Documents/Spécialité Data Science/Practical Machine Learning/Projects/Personal-Activity-Prediction")
setwd("C:/Users/HomeUser/Documents/Spécialité Data Science/Practical Machine Learning/Projects/Personal-Activity-Prediction")
setwd("C:/Users/HomeUser/Documents/Spécialité Data Science/Practical Machine Learning/Projects/Personal-Activity-Prediction")
train <- read.csv("pml-training.csv", header = T
, sep=","
,na.strings = c(NA,"","NA","<NA>")
#, comment.char = ""
)
test <- read.csv("pml-testing.csv", header = T
, sep=","
,na.strings = c(NA,"","NA","<NA>")
#, comment.char = ""
)
print(object.size(train),unit='Mb')
head(train)
names(train)
summary(train)
str(train)
# discard NAs
NAs <- apply(train,2,function(x) {sum(is.na(x))})
validData <- train[,which(NAs <= (10/100)*nrow(train))]
validData$X = NULL
# make training set
library(caret)
set.seed(123)
trainIndex <- createDataPartition(y = validData$classe, p=0.2,list=FALSE)
training <- validData[trainIndex,]
# make model
fitControl <- trainControl(method = "cv", number = 4) #speed up VS default parameter!
gbmGrid <-  expand.grid(interaction.depth = 1:4,
n.trees = (1:5)*50,
shrinkage = c(0.1,0.5)
system.time(
modFit <- train(classe ~.,data = training,method="gbm"
,trControl = fitControl
,verbose = FALSE
,tuneGrid = gbmGrid
)
)
modFit
modFit$finalModel
#modFit$control$index
#modFit$resample
summary(modFit)
#summary(modFit)["user_name",]
length(unique(training$raw_timestamp_part_1))
sort(apply(training,2,function(x) {length(unique(x))}))
head(cbind(training$raw_timestamp_part_1,training$classe),50)
testing <- validData[-trainIndex,]
pred <- predict(modFit,testing)
table(pred,testing$classe)
confusionMatrix(pred, testing$classe)
gbmGrid <-  expand.grid(interaction.depth = 1:4,
n.trees = (1:5)*50,
shrinkage = c(0.1,0.5)
)
system.time(
modFit <- train(classe ~.,data = training,method="gbm"
,trControl = fitControl
,verbose = FALSE
,tuneGrid = gbmGrid
)
)
modFit
modFit$finalModel
#modFit$control$index
#modFit$resample
summary(modFit)
#summary(modFit)["user_name",]
length(unique(training$raw_timestamp_part_1))
sort(apply(training,2,function(x) {length(unique(x))}))
head(cbind(training$raw_timestamp_part_1,training$classe),50)
testing <- validData[-trainIndex,]
pred <- predict(modFit,testing)
table(pred,testing$classe)
confusionMatrix(pred, testing$classe)
sapply(ls(), function(x) {
print(object.size(x),units='Mb')})
rm()
gc()
gbmGrid <-  expand.grid(interaction.depth = 1:4,
n.trees = (1:5)*50,
shrinkage = c(0.1,0.5)
)
system.time(
modFit <- train(classe ~.,data = training,method="gbm"
,trControl = fitControl
,verbose = FALSE
,tuneGrid = gbmGrid
,allowParallel = T
)
)
warnings()
system.time(
modFit <- train(classe ~.,data = training,method="gbm"
,trControl = fitControl
,verbose = FALSE
,tuneGrid = gbmGrid
#,allowParallel = T
)
)
gbmGrid <-  expand.grid(interaction.depth = 1:3,
n.trees = (1:4)*50,
#shrinkage = c(0.1,0.5)
shrinkage = 0.1
)
system.time(
modFit <- train(classe ~.,data = training,method="gbm"
,trControl = fitControl
,verbose = FALSE
,tuneGrid = gbmGrid
#,allowParallel = T
)
)
gbmGrid <-  expand.grid(interaction.depth = 1:3,
n.trees = (1:4)*50,
shrinkage = c(0.1,0.5)
#shrinkage = 0.1
)
system.time(
modFit <- train(classe ~.,data = training,method="gbm"
,trControl = fitControl
,verbose = FALSE
,tuneGrid = gbmGrid
#,allowParallel = T
)
)
