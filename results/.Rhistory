plot(training$Superplasticizer)
qplot(training$Superplasticizer)
qplot(log(1+training$Superplasticizer)
)
qplot(log(1+training$Superplasticizer))
qplot(training$Superplasticizer)
qplot(log(1+training$Superplasticizer))
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
names(training)
grep("IL*",names(training),value=T)
grep("IL",names(training),value=T)
grep("^IL",names(training),value=T)
grep("^IL",names(training),value=T,fixed=T)
grep("IL",names(training),value=T,fixed=T)
grep("^IL",names(training),value=T)
grep("^IL",names(training))
ILindexes <- grep("^IL",names(training))
preProc <- preProcess(training[,ILindexes],method="pca",thresh=0.9)
trainPC <- predict(preProc,training[,ILindexes])
preProc <- preProcess(training[,ILindexes],method="pca",thresh=0.5)
preProc <- preProcess(training[,ILindexes],method="pca",thresh=0.99)
trainPC <- predict(preProc,training[,ILindexes])
preProc <- preProcess(training[,ILindexes],method="pca",thresh=0.9)
trainPC <- predict(preProc,training[,ILindexes])
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ILindexes <- grep("^IL",names(training))
ILtraining = training[,ILindexes]
preProc <- preProcess(ILtraining,method="pca",thresh=0.8)
trainPC <- predict(preProc,ILtraining)
modelPCFit <- train(training$diagnosis ~ .,method="glm",data=trainPC)
library(caret)
modelPCFit <- train(training$diagnosis ~ .,method="glm",data=trainPC)
library(e1071)
trainPC <- predict(preProc,ILtraining)
install.packages("e1071")
library(e1071)
modelPCFit <- train(training$diagnosis ~ .,method="glm",data=trainPC)
modelFit <- train(training$diagnosis ~ .,method="glm",data=ILtraining)
testPC <- predict(preProc,testing[,ILindexes])
testPC <- predict(preProc,testing[,ILindexes])
confusionMatrix(testing$diagnosis,predict(modelPCFit,testPC))
modelFit <- train(training$diagnosis ~ .,method="glm",data=ILtraining)
modelFit <- train(training$diagnosis ~ .,method="glm",data=ILtraining)
confusionMatrix(testing$diagnosis,predict(modelFit,testing[,ILindexes]))
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
names(training)
ILindexes <- grep("^IL",names(training))
preProc <- preProcess(training[,ILindexes],method="pca",thresh=0.8)
trainPC <- predict(preProc,training[,ILindexes])
# We start by defining the quantities whose value is known.
S0=8; # Equity at time 0, i.e. today.
sigmaS=0.75; # Instantaneous volatility of equity
r=0.06; # Risk-free rate on the market
T=1; # Maturity
B=7.5; # Face value of debt obligation, i.e. liabilities.
# We then need to write down the function we will minimize in order to obtain
# V0, the value of companyâs assets today, and sigmaV, assetsâ volatility
# To write a function in R, we use the command âfunctionâ. Refer to the R intro
# for more details.
Merton_solve=function(parm){
V0=parm[1] #initial value for V0
sigmaV=parm[2] #initial value for sigmaV
# And now, all the quantities we have seen in the slides.
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
F=V0*pnorm(d1)-B*exp(-r*T)*pnorm(d2)-S0
G=pnorm(d1)*sigmaV*V0-sigmaS*S0
#  Finally the result of our function:
return(F^2+G^2)
}
# For the minimization step, in order to find V_0 and sigma_V,
# we need to specify two initial values.
# Let us choose V_0=13, and sigmaV=0.5.
# Other plausible values can obviously be chosen.
solutions=optim(c(V0=13,sigmaV=0.5),Merton_solve)
# What are the estimated values?
V0=solutions$par[1]
sigmaV=solutions$par[2]
# Let us compute d1 and d2 explicitly:
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
# And finally the probability of default in one year for our company
pnorm(-d2)
# We start by defining the quantities whose value is known.
S0=8; # Equity at time 0, i.e. today.
sigmaS=0.75; # Instantaneous volatility of equity
r=0.06; # Risk-free rate on the market
T=1; # Maturity
B=14; # Face value of debt obligation, i.e. liabilities.
# We then need to write down the function we will minimize in order to obtain
# V0, the value of companyâs assets today, and sigmaV, assetsâ volatility
# To write a function in R, we use the command âfunctionâ. Refer to the R intro
# for more details.
Merton_solve=function(parm){
V0=parm[1] #initial value for V0
sigmaV=parm[2] #initial value for sigmaV
# And now, all the quantities we have seen in the slides.
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
F=V0*pnorm(d1)-B*exp(-r*T)*pnorm(d2)-S0
G=pnorm(d1)*sigmaV*V0-sigmaS*S0
#  Finally the result of our function:
return(F^2+G^2)
}
# For the minimization step, in order to find V_0 and sigma_V,
# we need to specify two initial values.
# Let us choose V_0=13, and sigmaV=0.5.
# Other plausible values can obviously be chosen.
solutions=optim(c(V0=13,sigmaV=0.5),Merton_solve)
# What are the estimated values?
V0=solutions$par[1]
sigmaV=solutions$par[2]
# Let us compute d1 and d2 explicitly:
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
# And finally the probability of default in one year for our company
pnorm(-d2)
# We start by defining the quantities whose value is known.
S0=11; # Equity at time 0, i.e. today.
sigmaS=0.7; # Instantaneous volatility of equity
r=0.06; # Risk-free rate on the market
T=1; # Maturity
B=18; # Face value of debt obligation, i.e. liabilities.
# We then need to write down the function we will minimize in order to obtain
# V0, the value of companyâs assets today, and sigmaV, assetsâ volatility
# To write a function in R, we use the command âfunctionâ. Refer to the R intro
# for more details.
Merton_solve=function(parm){
V0=parm[1] #initial value for V0
sigmaV=parm[2] #initial value for sigmaV
# And now, all the quantities we have seen in the slides.
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
F=V0*pnorm(d1)-B*exp(-r*T)*pnorm(d2)-S0
G=pnorm(d1)*sigmaV*V0-sigmaS*S0
#  Finally the result of our function:
return(F^2+G^2)
}
# For the minimization step, in order to find V_0 and sigma_V,
# we need to specify two initial values.
# Let us choose V_0=13, and sigmaV=0.5.
# Other plausible values can obviously be chosen.
solutions=optim(c(V0=13,sigmaV=0.5),Merton_solve)
# What are the estimated values?
V0=solutions$par[1]
sigmaV=solutions$par[2]
# Let us compute d1 and d2 explicitly:
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
# And finally the probability of default in one year for our company
pnorm(-d2)
# We start by defining the quantities whose value is known.
S0=11; # Equity at time 0, i.e. today.
sigmaS=0.7; # Instantaneous volatility of equity
r=0.06; # Risk-free rate on the market
T=1; # Maturity
B=15; # Face value of debt obligation, i.e. liabilities.
# We then need to write down the function we will minimize in order to obtain
# V0, the value of companyâs assets today, and sigmaV, assetsâ volatility
# To write a function in R, we use the command âfunctionâ. Refer to the R intro
# for more details.
Merton_solve=function(parm){
V0=parm[1] #initial value for V0
sigmaV=parm[2] #initial value for sigmaV
# And now, all the quantities we have seen in the slides.
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
F=V0*pnorm(d1)-B*exp(-r*T)*pnorm(d2)-S0
G=pnorm(d1)*sigmaV*V0-sigmaS*S0
#  Finally the result of our function:
return(F^2+G^2)
}
# For the minimization step, in order to find V_0 and sigma_V,
# we need to specify two initial values.
# Let us choose V_0=13, and sigmaV=0.5.
# Other plausible values can obviously be chosen.
solutions=optim(c(V0=13,sigmaV=0.5),Merton_solve)
# What are the estimated values?
V0=solutions$par[1]
sigmaV=solutions$par[2]
# Let us compute d1 and d2 explicitly:
d1=(log(V0/B)+(r+sigmaV^2/2)*T)/(sigmaV*sqrt(T))
d2=d1-sigmaV*sqrt(T)
# And finally the probability of default in one year for our company
pnorm(-d2)
qnorm(2/100)
pnorm(2.05)
pnorm(-2.05)
d1=2.053749+0.3
d1
sigmaV=0.3
T=1
B=15
r=0.06
B*exp((d1*sigmaV*sqrt(T))-(r+sigmaV^2/2)*T))
B*exp((d1*sigmaV*sqrt(T))-(r+sigmaV^2/2)*T)
install.packages("CreditMetrics")
qnorm(0.0019)
qnorm(0.9995)
50*0.2*((pnorm((qnorm(0.5)))-0.005)
)
50*0.2*(pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.9999)/sqrt(1-0.15)))-0.005)
50*0.2*(pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.9999)/sqrt(1-0.15)))-0.005)*0.08
50*0.2*(pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.9999))/sqrt(1-0.15))-0.005)
50*0.2*(pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.9999))/sqrt(1-0.15))-0.005)*0.08
12.5*50*0.2*(pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.9999))/sqrt(1-0.15))-0.005)
pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.9999))/sqrt(1-0.15))
pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.999))/sqrt(1-0.15))
1/0.08
50*0.2*(pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.999))/sqrt(1-0.15))-0.005)
50*0.2*(pnorm((qnorm(0.005)+sqrt(0.15)*qnorm(0.999))/sqrt(1-0.15))-0.005)*12.5
qnorm(0.06)
qnorm(1-0.06-2.82)
qnorm(0.0006)
qnorm(1-0.0006-0.00282)
qnorm(1-0.0006)
qnorm(1-0.0006-0.0282)
rho_retail = 0.03+0.13*exp(-35*0.01)
wcdr=pnorm((qnorm(0.01)+sqrt(rho_retail)*qnorm(0.999))/sqrt(1-rho_retail))
wcdr
200*0.7*(wcdr-0.01)
200*0.7*(wcdr-0.01)*12.5
rho_corp = 0.12*(1+exp(-50*0.003))
wcdr=pnorm((qnorm(0.003)+sqrt(rho_corp)*qnorm(0.999))/sqrt(1-rho_corp))
rho_corp
wcdr
b=(0.11852 - 0.05478*log(0.003))^2
ma=(1+(3-2.5)*b)/(1-1.5*b)
500*0.6*(wcdr-0.003)*ma
500*0.6*(wcdr-0.003)*ma*12.5
(1/100)/0.4
(7/100)/0.4
(0.6/100)/0.4
(0.025*10-5*0.015)/5
# Let's generate 100 losses
losses=rlnorm(100,1,2)
# or
# losses=read.table("losses_svar.txt",head=T),
# and then
# losses=losses$x
# if you use my data.
# We can plot the losses
hist(losses, col=3)
quantile(losses,0.95,type=3)
losses=sort(losses)
# Now we select the worst 50%
worst_losses=losses[51:100]
# The 95% S-VaR is therefore
quantile(worst_losses,0.95,type=3)
quantile(losses,0.975,type=3)
time()
date()
library(caret)
setwd("C:/Users/HomeUser/Documents/Spécialité Data Science/Practical Machine Learning/Projects/Personal-Activity-Prediction")
rawtrain <- read.csv("pml-training.csv", header = T
, sep=","
,na.strings = c(NA,"","NA","<NA>")
#, comment.char = ""
)
rawtest <- read.csv("pml-testing.csv", header = T
, sep=","
,na.strings = c(NA,"","NA","<NA>")
#, comment.char = ""
)
print(object.size(rawtrain),unit='Mb')
head(rawtrain)
names(rawtrain)
summary(rawtrain)
str(rawtrain)
#PREPROCESSING
# discard unuseful variables
##NAs
NAs <- apply(rawtrain,2,function(x) {sum(is.na(x))}) #count NAs for each variable
##Other variables
#removeIndex <- grep("X|window|timestamp|user_name",names(rawtrain),value=T) #assume...
#removeIndex
#qplot(rawtrain$num_window,rawtrain$classe)
removeIndex <- grep("X|new_window|timestamp|user_name",names(rawtrain),value=F)
keepIndex <- grep("_x$|_y$|_z$",names(rawtrain),value=F)
##Remove all together here
fulltrain <- rawtrain[,-c(which(NAs > (10/100)*nrow(rawtrain)),removeIndex)] #-> 54 predictors
#fulltrain <- rawtrain[,c(intersect(which(NAs <= (10/100)*nrow(rawtrain)),keepIndex),length(rawtrain))] #-> 37 predictors
test <- rawtest[,c(intersect(which(NAs <= (10/100)*nrow(rawtrain)),keepIndex))]
##2nd (optional) pass: correlations NOT USED HERE
M <- abs(cor(fulltrain[,-which(names(fulltrain)=="classe")]))
diag(M) <- 0
w <- data.frame(which(M > 0.9,arr.ind=T))
w
apply(w,1,function(x) #todo, doesn't work
{names(fulltrain)[c(x$row,x$col)]}
)
##2nd (optional) pass: PCA both on train and validation data
fulltrain.preproc <- preProcess(fulltrain[,-which(names(fulltrain)=="classe")], method='pca', thresh=0.99)
fulltrain.pca <- predict(fulltrain.preproc, fulltrain[,-which(names(fulltrain)=="classe")])
test.preproc <- preProcess(test, method='pca', thresh=0.99)
test.pca <- predict(test.preproc, test)
##Split data into train + validation datas
trainIndex <- createDataPartition(y = fulltrain$classe, p=0.7,list=FALSE)
part_training <- fulltrain[trainIndex,]
part_training.pca <- fulltrain.pca[trainIndex,]
part_validating <- fulltrain[-trainIndex,] #todo: change name to validation...
part_validating.pca <- fulltrain.pca[-trainIndex,] #todo: change name to validation...
# TRAINING
set.seed(123)
fitControl <- trainControl(method = "cv", number = 4) #speed up VS default parameter!
gbmGrid <-  expand.grid(interaction.depth = 1:3,
n.trees = (1:4)*50,
#shrinkage = c(0.1,0.5)
shrinkage = 0.1
)
system.time(
#modFit <- train(classe ~.
modFit <- train(fulltrain[trainIndex,]$classe ~. #fulltrain instead of directly classe~. for compatibility with .pca datas
,data = part_training #CHOOSE part_training.pca or part_training !
,method="gbm"
,trControl = fitControl
,verbose = FALSE
,tuneGrid = gbmGrid #TO COMMENT IF METHOD IS NOT GBM !
#,allowParallel = T
)
)
modFit
modFit$finalModel
#modFit$control$index
#modFit$resample
summary(modFit)
#summary(modFit)["user_name",]
length(unique(part_training$raw_timestamp_part_1))
sort(apply(part_training,2,function(x) {length(unique(x))}))
head(cbind(part_training$raw_timestamp_part_1,part_training$classe),50)
#VALIDATING MODEL
pred.part_validating <- predict(modFit,part_validating) #CHOOSE part_validating.pca or part_validating !
table(pred.part_validating,fulltrain[-trainIndex,]$classe)
confusionMatrix(pred.part_validating, part_validating$classe)
#TESTING MODEL
pred.test <- predict(modFit,test) #CHOOSE test.pca or test !
pred.test
#CLEAN MEMORY
sapply(ls(), function(x) {
print(object.size(x),units='Mb')})
rm()
gc()
#PUT ANSWERS TO FILES
answers = as.character(pred.test)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
setwd("C:/Users/HomeUser/Documents/Spécialité Data Science/Practical Machine Learning/Projects/Personal-Activity-Prediction/results")
pml_write_files(answers)
test <- rawtest[,-c(which(NAs > (10/100)*nrow(rawtrain)),removeIndex)]
#VALIDATING MODEL
pred.part_validating <- predict(modFit,part_validating) #CHOOSE part_validating.pca or part_validating !
table(pred.part_validating,fulltrain[-trainIndex,]$classe)
confusionMatrix(pred.part_validating, part_validating$classe)
#TESTING MODEL
pred.test <- predict(modFit,test) #CHOOSE test.pca or test !
pred.test
#CLEAN MEMORY
sapply(ls(), function(x) {
print(object.size(x),units='Mb')})
rm()
gc()
#PUT ANSWERS TO FILES
answers = as.character(pred.test)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
setwd("C:/Users/HomeUser/Documents/Spécialité Data Science/Practical Machine Learning/Projects/Personal-Activity-Prediction/results")
pml_write_files(answers)
require(gbm) || install.packages("gbm")
require(randomForest) || install.packages("randomForest")
require(e1071) || install.packages("e1071")
require(caret) || install.packages("caret")
library(caret)
date()
#download.file(fileUrl, destfile = "./data/w1.1.csv") #erreur avec: , method = "curl"
setwd("/home/rstudio/projects/Personal-Activity-Prediction")
rawtrain <- read.csv("pml-training.csv", header = T
, sep=","
,na.strings = c(NA,"","NA","<NA>")
#, comment.char = ""
)
rawtest <- read.csv("pml-testing.csv", header = T
, sep=","
,na.strings = c(NA,"","NA","<NA>")
#, comment.char = ""
)
print(object.size(rawtrain),unit='Mb')
head(rawtrain)
names(rawtrain)
summary(rawtrain)
str(rawtrain)
#PREPROCESSING
# discard unuseful variables
##NAs
NAs <- apply(rawtrain,2,function(x) {sum(is.na(x))}) #count NAs for each variable
##Other variables
#removeIndex <- grep("X|window|timestamp|user_name",names(rawtrain),value=T) #assume...
#removeIndex
#qplot(rawtrain$num_window,rawtrain$classe)
removeIndex <- grep("X|new_window|timestamp|user_name",names(rawtrain),value=F)
keepIndex <- grep("_x$|_y$|_z$",names(rawtrain),value=F)
##Remove all together here
###Option 1
fulltrain <- rawtrain[,-c(which(NAs > (10/100)*nrow(rawtrain)),removeIndex)] #-> 54 predictors
test <- rawtest[,-c(which(NAs > (10/100)*nrow(rawtrain)),removeIndex)]
###Option 2
#fulltrain <- rawtrain[,c(intersect(which(NAs <= (10/100)*nrow(rawtrain)),keepIndex),length(rawtrain))] #-> 37 predictors
#test <- rawtest[,c(intersect(which(NAs <= (10/100)*nrow(rawtrain)),keepIndex))]
##2nd (optional) pass: correlations NOT USED HERE
M <- abs(cor(fulltrain[,-which(names(fulltrain)=="classe")]))
diag(M) <- 0
w <- data.frame(which(M > 0.9,arr.ind=T))
w
apply(w,1,function(x) #todo, doesn't work
{names(fulltrain)[c(x$row,x$col)]}
)
##2nd (optional) pass: PCA both on train and validation data
fulltrain.preproc <- preProcess(fulltrain[,-which(names(fulltrain)=="classe")], method='pca', thresh=0.99)
fulltrain.pca <- predict(fulltrain.preproc, fulltrain[,-which(names(fulltrain)=="classe")])
test.preproc <- preProcess(test, method='pca', thresh=0.99)
test.pca <- predict(test.preproc, test)
##Split data into train + validation datas
trainIndex <- createDataPartition(y = fulltrain$classe, p=0.7,list=FALSE)
part_training <- fulltrain[trainIndex,]
part_training.pca <- fulltrain.pca[trainIndex,]
part_validating <- fulltrain[-trainIndex,] #todo: change name to validation...
part_validating.pca <- fulltrain.pca[-trainIndex,] #todo: change name to validation...
# TRAINING
set.seed(123)
fitControl <- trainControl(method = "cv", number = 4) #speed up VS default parameter!
gbmGrid <-  expand.grid(interaction.depth = 1:3,
n.trees = (1:4)*50,
#shrinkage = c(0.1,0.5)
shrinkage = 0.1
)
system.time(
#modFit <- train(classe ~.
modFit <- train(fulltrain[trainIndex,]$classe ~. #fulltrain instead of directly classe~. for compatibility with .pca datas
,data = part_training #CHOOSE part_training.pca or part_training !
,method="rpart"
#,trControl = fitControl
,verbose = FALSE
#,tuneGrid = gbmGrid #TO COMMENT IF METHOD IS NOT GBM !
#,allowParallel = T
)
)
modFit
modFit$finalModel
#modFit$control$index
#modFit$resample
summary(modFit)
#summary(modFit)["user_name",]
length(unique(part_training$raw_timestamp_part_1))
sort(apply(part_training,2,function(x) {length(unique(x))}))
head(cbind(part_training$raw_timestamp_part_1,part_training$classe),50)
#VALIDATING MODEL
pred.part_validating <- predict(modFit,part_validating) #CHOOSE part_validating.pca or part_validating !
table(pred.part_validating,fulltrain[-trainIndex,]$classe)
confusionMatrix(pred.part_validating, part_validating$classe)
#TESTING MODEL
pred.test <- predict(modFit,test) #CHOOSE test.pca or test !
pred.test
#CLEAN MEMORY
sapply(ls(), function(x) {
print(object.size(x),units='Mb')})
rm()
gc()
#PUT ANSWERS TO FILES
answers = as.character(pred.test)
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
setwd("/home/rstudio/projects/Personal-Activity-Prediction/results")
pml_write_files(answers)
